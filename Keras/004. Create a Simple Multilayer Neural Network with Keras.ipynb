{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras는 딥러닝 모델을 개발하고 평가할 수있는 강력하고 사용하기 쉬운 Python 라이브러리다. 효율적인 수치 계산 라이브러리인 Theano와 TensorFlow를 백엔드에 두고, 단 줄의 코드로 신경망 모델을 구축하고 학습할 수 있다.\n",
    "\n",
    "여기에서는 Keras를 사용하여 간단한 신경망 모델을 만드는 방법을 살펴보고자 한다. 구체적으로, 다음과 같은 사항에 대해 이야기할 것이다.\n",
    "\n",
    "- Keras와 함께 사용할 준비가 된 CSV 데이터 세트를 로드하는 방법.\n",
    "- Keras에서 Multilayer perceptron 모델을 정의하고 컴파일하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "\n",
    "본 튜토리얼에서는 다음과 같은 순서대로 진행할 것이다.\n",
    "\n",
    "1. 데이터 불러오기\n",
    "2. 모델 정의\n",
    "3. 모델 컴파일\n",
    "4. 모델 적합\n",
    "5. 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indians Onset of Diabetes Dataset\n",
    "\n",
    "이 실습을 위해, Pima Indians의 당뇨병 dataset을 사용할 것이다. 이 dataset은 UCI Machine Learning repository에서 구할 수 있다.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data \n",
    "\n",
    "Pima indians에 속한 환자의 의료 기록 데이터와, 5년 이내에 당뇨병 진단을 받았는지 여부를 설명하기 위한 데이터이다. 이 데이터가 어려가지 목적 하에서 활용될 수 있겠지만, 여기에서는 당뇨병 진단을 받은 경우를 1, 아닌 경우를 0으로 두고 binary classification을 할 것이다.\n",
    "\n",
    "이 데이터에서 제공하는 변수는 다음과 같다. 모든 데이터는 수치 데이터이다.\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index.\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class, onset of diabetes within five years.\n",
    "\n",
    "총 768개의 인스턴스가 존재하며, 처음 5번째 행은 다음과 같은 모습이다.\n",
    "\n",
    "6,148,72,35,0,33.6,0.627,50,1\n",
    "\n",
    "1,85,66,29,0,26.6,0.351,31,0\n",
    "\n",
    "8,183,64,0,0,23.3,0.672,32,1\n",
    "\n",
    "1,89,66,23,94,28.1,0.167,21,0\n",
    "\n",
    "0,137,40,35,168,43.1,2.288,33,1\n",
    "\n",
    "모든 변수가 수치형 데이터라는 점에서, 기본적으로 수치형 데이터의 입출력을 기대하는 신경망에 직접 적합하기에 매우 용이할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "Stochastic process를 사용하는 기계학습 알고리즘을 사용할 때마다, 고정 시드 값으로 난수 생성 프로그램을 초기화하는 것이 좋다. 이렇게 하면, 동일한 코드를 반복해서 실행할 때마다 언제나 동일한 결과를 얻을 수 있다. 특히 이 방법은, 다른 사람들에게 결과를 보여주거나, 다른 알고리즘과 성능을 비교하거나, 코드를 디버깅할 때 유용하다.\n",
    "\n",
    "파이썬에서는 numpy의 random.seed 메서드를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# 고정 시드 값으로 초기화\n",
    "numpy.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Pima Indians dataset을 불러오자. CSV 파일은 numpy 라이브러리의 loadtxt() 함수를 사용하여 직접 불러올 수 있다. Pima Indians dataset에는 8개의 입력 변수와 1개의 출력 변수(마지막 열)가 있다. 일단 dataset을 불러오면, 입력변수(X)와 출력변수(Y)를 나눌 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pima Indians Diabetes 데이터 불러오기\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# 입력 변수와 출력 변수를 구분하기\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "\n",
    "Keras의 모델은 일련의 레이어로 정의된다. 네트워크 토폴로지에 만족할 때까지 순차 모델을 만들고 레이어를 하나씩 추가한다. 첫 번째로 확인해야할 것은, 입력 레이어에 올바른 입력 수가 있는지 확인하는 것이다. 이것은 input_dim 인수로 첫 번째 레이어를 만들 때 지정할 수 있다. (그에 앞서, 해당 변수의 dimension을 확인하기 위해서는 shape() 메서드를 사용할 수 있다.)\n",
    "\n",
    "적절한 레이어의 수와 유형을 어떻게 알 수 있을까? 이것은 매우 어려운 질문이다. 보통 우리는 이 레이어 수와 유형을 계속 변경하고 시행 착오를 겪으면서 최상의 네트워크 구조를 발견해내는 식이다. 일반적으로 문제의 구조를 파악하기 위해서는 충분한 수의 네트워크가 필요하다.\n",
    "\n",
    "이 예제에서는 3 개의 레이어가 있는 완전히 연결된(fully connected) 네트워크 구조를 사용하기로 한다.\n",
    "\n",
    "완전히 연결된(fully connected) 레이어는 Dense 클래스를 사용하여 정의된다. 레이어의 뉴런 수(노드 수)를 첫 번째 인수로 지정하고, 두 번째 인수로서 초기화 메서드를 init으로 지정하고, activation 인수를 사용하여 활성화 함수를 지정할 수 있다. 이 경우 Keras의 기본 가중치 초기화는 Uniform[0, 0.5]에서 생성 된 작은 난수로 네트워크 가중치를 초기화한다. 이와 다른 전통적인 대안으로는 Gaussian 분포로부터 생성하기도 한다.\n",
    "\n",
    "처음 두 레이어에는 RELU 활성화 함수를 사용하고 출력 레이어에는 Sigmoid 활성화 함수를 사용하였다. 이전에는 Sigmoid 혹은 Tan 활성화 함수가 모든 레이어에서 선호되어왔지만, 오늘날은 조금 다양한 함수를 활용하는 추세이고 각 활성화 함수마다 장단점을 인지하며 사용하려고도 하는 편이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 설명한 모델은, 아래 그림으로 표현할 수 있다.\n",
    "\n",
    "<img src=\"004_001.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 컴파일\n",
    "\n",
    "모델이 정의되었으므로 이제 컴파일 할 수 있다. 모델을 컴파일할 때는 백엔드에서 Theano 또는 TensorFlow와 같은 효율적인 수치 라이브러리를 사용하게 된다. 백엔드는 학습을 위해 네트워크를 표현하고 하드웨어에서 예측을 수행하는 가장 좋은 방법을 자동으로 선택한다. 컴파일 할 때, 네트워크를 학습에 필요한 몇 가지 추가 속성을 지정해야한다.\n",
    "\n",
    "우리는 가중치 셋을 평가하는 데 사용할 손실 함수, 네트워크의 다른 가중치를 탐색하는 데 사용되는 최적화 알고리즘 및 학습 중에 수집하고 보고하고자하는 선택적인 측정 기준을 지정해야 한다. 이 경우 여기에서는 logarithmic loss를 사용할 것인데, 이진 분류 문제는 Keras에서 binary_crossentropy로 정의한다. 또 여기에서는 효율적인 그라디언트 디센트 알고리즘 중 하나인 Adam을 사용할 것이다. 마지막으로 분류 문제이므로 분류 정확도를 확인하기 위해 accuracy를 수집하여 보고하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 적합 (fit model)\n",
    "\n",
    "우리는 모델을 정의하고 효율적인 계산을 위한 준비를 끝냈다. 이제는 데이터에로부터 모델을 fitting할 차례이다. fit() 함수를 호출하여 로드 된 데이터에 모델을 적용하거나 조정할 수 있습니다.\n",
    "\n",
    "학습 과정은 epochs라는 '데이터 세트를 통해 고정 된 반복 횟수' 동안 실행되며, epochs 인수를 사용하여 얼마나 반복할 것인지를 지정해야 한다. 네트워크에서 batch_size 인수를 조정하여, 데이터를 한 번에 처리하지 않고 일부분씩 나누어 처리할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 3s - loss: 3.7357 - acc: 0.5990     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.9400 - acc: 0.5911     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.7509 - acc: 0.6393     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.7113 - acc: 0.6589     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6799 - acc: 0.6784     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6493 - acc: 0.6823     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6486 - acc: 0.6706     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6359 - acc: 0.6875     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6230 - acc: 0.6888     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6285 - acc: 0.6771     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6448 - acc: 0.6732     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6378 - acc: 0.6693     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6248 - acc: 0.6771     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6175 - acc: 0.7005     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6017 - acc: 0.6953     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.5876 - acc: 0.7005     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5846 - acc: 0.6992     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5999 - acc: 0.6875     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5799 - acc: 0.7096     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5790 - acc: 0.7227     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5683 - acc: 0.7174     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5812 - acc: 0.6992     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5729 - acc: 0.7161     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5675 - acc: 0.7318     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5573 - acc: 0.7331     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5703 - acc: 0.7070     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5553 - acc: 0.7227     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5551 - acc: 0.7318     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5746 - acc: 0.7174     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5612 - acc: 0.7214     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5686 - acc: 0.7161     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5636 - acc: 0.7135     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5524 - acc: 0.7161     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5499 - acc: 0.7305     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5494 - acc: 0.7227     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5645 - acc: 0.7083     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5334 - acc: 0.7370     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5410 - acc: 0.7227     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5462 - acc: 0.7227     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5438 - acc: 0.7227     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5424 - acc: 0.7318     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5368 - acc: 0.7448     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5297 - acc: 0.7500     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5333 - acc: 0.7448     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5314 - acc: 0.7578     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5273 - acc: 0.7461     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5311 - acc: 0.7383     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5330 - acc: 0.7396     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5339 - acc: 0.7409     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5269 - acc: 0.7383     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5288 - acc: 0.7435     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5295 - acc: 0.7435     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5364 - acc: 0.7461     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5374 - acc: 0.7266     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5222 - acc: 0.7526     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5292 - acc: 0.7409     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5311 - acc: 0.7357     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5231 - acc: 0.7539     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5122 - acc: 0.7630     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5331 - acc: 0.7331     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5276 - acc: 0.7331     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5178 - acc: 0.7552     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5448 - acc: 0.7370     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5310 - acc: 0.7422     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5208 - acc: 0.7461     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5067 - acc: 0.7513     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5157 - acc: 0.7331     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5131 - acc: 0.7526     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5137 - acc: 0.7500     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5368 - acc: 0.7240     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5168 - acc: 0.7383     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5171 - acc: 0.7474     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.5163 - acc: 0.7461     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5103 - acc: 0.7617     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5093 - acc: 0.7578     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5118 - acc: 0.7552     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5156 - acc: 0.7630     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.5114 - acc: 0.7487     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5137 - acc: 0.7357     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5115 - acc: 0.7552     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5068 - acc: 0.7656     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5043 - acc: 0.7526     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.4996 - acc: 0.7578     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4984 - acc: 0.7604     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5054 - acc: 0.7487     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5050 - acc: 0.7487     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4990 - acc: 0.7500     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.5017 - acc: 0.7656     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.5060 - acc: 0.7669     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5084 - acc: 0.7526     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5033 - acc: 0.7578     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.5059 - acc: 0.7435     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4978 - acc: 0.7669     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4988 - acc: 0.7682     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.5028 - acc: 0.7487     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4908 - acc: 0.7682     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4993 - acc: 0.7747     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7604     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4913 - acc: 0.7604     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4838 - acc: 0.7773     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4888 - acc: 0.7786     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4974 - acc: 0.7630     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4980 - acc: 0.7591     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4903 - acc: 0.7865     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.5291 - acc: 0.7448     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4931 - acc: 0.7721     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4891 - acc: 0.7734     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4997 - acc: 0.7708     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4890 - acc: 0.7656     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4900 - acc: 0.7760     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4836 - acc: 0.7773     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4926 - acc: 0.7708     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4934 - acc: 0.7669     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4909 - acc: 0.7617     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4917 - acc: 0.7747     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4944 - acc: 0.7669     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4917 - acc: 0.7643     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4902 - acc: 0.7760     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4831 - acc: 0.7708     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4935 - acc: 0.7747     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4935 - acc: 0.7708     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4863 - acc: 0.7799     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4807 - acc: 0.7721     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4833 - acc: 0.7734     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4873 - acc: 0.7852     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4804 - acc: 0.7852     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4909 - acc: 0.7682     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4729 - acc: 0.7760     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4821 - acc: 0.7682     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4754 - acc: 0.7865     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4824 - acc: 0.7708     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4830 - acc: 0.7813     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4829 - acc: 0.7721     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4851 - acc: 0.7773     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4786 - acc: 0.7813     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4738 - acc: 0.7786     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4692 - acc: 0.7786     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4802 - acc: 0.7813     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4655 - acc: 0.7930     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4827 - acc: 0.7813     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4742 - acc: 0.7839     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4840 - acc: 0.7721     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4765 - acc: 0.7708     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4770 - acc: 0.7773     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4905 - acc: 0.7643     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4933 - acc: 0.7695     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4834 - acc: 0.7799     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4734 - acc: 0.7773     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4773 - acc: 0.7630     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4749 - acc: 0.7695     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8ebcc9f98>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모형 적합\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서의 작업이, CPU 또는 GPU에서 이루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가\n",
    "\n",
    "우리는 전체 dataset으로부터 신경망을 학습시켰으므로, 이제 동일한 dataset을 이용하여 네트워크의 성능을 평가할 수 있다.\n",
    "\n",
    "(다만, 이렇게 하면 dataset이 얼마나 잘 모델링되었는지는 알 수 있지만, 알고리즘이 새 데이터에 대해서는 얼마나 잘 수행하는지 알 수 없다. 이를 위해 모델의 training 및 test를 위한 dataset을 따로 분리할 수 있다. 이에 대해서는 나중에 다룰 것이다.)\n",
    "\n",
    "모델의 evaluation() 함수를 사용하여 training dataset에서 모델을 평가하고, 모델을 학습하는 데 사용된 것과 동일한 입력 및 출력을 전달할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 46s\n",
      "acc: 64.97%\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 종합적으로 살펴보기.\n",
    "\n",
    "Keras에서 첫 번째 신경망 모델을 쉽게 만드는 방법을 살펴보았다. 지금까지 진행한 것을 종합하여 살펴보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 3.6741 - acc: 0.6055     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 3.2995 - acc: 0.6029     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 2.9690 - acc: 0.6042     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 1.9584 - acc: 0.5833     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 1.3228 - acc: 0.6302     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 1.1879 - acc: 0.6341     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 1.0442 - acc: 0.6302     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.9332 - acc: 0.6445     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.8814 - acc: 0.6289     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.7863 - acc: 0.6510     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.7357 - acc: 0.6602     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.7476 - acc: 0.6510     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.7131 - acc: 0.6315     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6851 - acc: 0.6549     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6776 - acc: 0.6549     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.6596 - acc: 0.6680     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.6633 - acc: 0.6654     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.7030 - acc: 0.6367     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.6606 - acc: 0.6536     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.6226 - acc: 0.6667     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.6374 - acc: 0.6771     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.6343 - acc: 0.6628     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.6071 - acc: 0.6693     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.6158 - acc: 0.6979     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5969 - acc: 0.6888     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5907 - acc: 0.6849     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5981 - acc: 0.6901     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5883 - acc: 0.7031     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5774 - acc: 0.7057     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5973 - acc: 0.6927     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5938 - acc: 0.6992     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5866 - acc: 0.6953     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5784 - acc: 0.6979     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5863 - acc: 0.6888     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5730 - acc: 0.7031     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5745 - acc: 0.7057     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5680 - acc: 0.7148     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5708 - acc: 0.7240     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5593 - acc: 0.7122     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5682 - acc: 0.7031     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5681 - acc: 0.7253     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5529 - acc: 0.7331     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5728 - acc: 0.7005     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5481 - acc: 0.7174     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5760 - acc: 0.7201     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5888 - acc: 0.7005     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5583 - acc: 0.7383     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5872 - acc: 0.7018     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5707 - acc: 0.7253     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5513 - acc: 0.7240     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5537 - acc: 0.7174     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5566 - acc: 0.7292     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5478 - acc: 0.7292     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5513 - acc: 0.7461     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5565 - acc: 0.7201     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5624 - acc: 0.7253     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5601 - acc: 0.7096     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5417 - acc: 0.7461     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5275 - acc: 0.7578     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5313 - acc: 0.7422     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5467 - acc: 0.7266     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5464 - acc: 0.7188     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5441 - acc: 0.7461     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5354 - acc: 0.7357     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5481 - acc: 0.7292     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5354 - acc: 0.7240     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5536 - acc: 0.7305     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5335 - acc: 0.7435     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5357 - acc: 0.7227     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5282 - acc: 0.7357     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5444 - acc: 0.7305     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5313 - acc: 0.7409     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.5205 - acc: 0.7331     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5360 - acc: 0.7396     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5231 - acc: 0.7513     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5211 - acc: 0.7643     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5121 - acc: 0.7526     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.5527 - acc: 0.7357     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5147 - acc: 0.7500     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5181 - acc: 0.7604     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5276 - acc: 0.7474     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5094 - acc: 0.7643     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.5234 - acc: 0.7370     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.5090 - acc: 0.7578     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5115 - acc: 0.7487     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5063 - acc: 0.7539     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.5445 - acc: 0.7174     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.5188 - acc: 0.7539     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.5121 - acc: 0.7578     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5272 - acc: 0.7474     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5242 - acc: 0.7383     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.4988 - acc: 0.7695     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.5079 - acc: 0.7565     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.5088 - acc: 0.7409     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.5036 - acc: 0.7539     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.5121 - acc: 0.7500     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4996 - acc: 0.7500     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.5040 - acc: 0.7578     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.5050 - acc: 0.7591     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.5024 - acc: 0.7643     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.5013 - acc: 0.7539     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4992 - acc: 0.7630     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.5064 - acc: 0.7526     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4985 - acc: 0.7617     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.5177 - acc: 0.7422     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.5137 - acc: 0.7474     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.5166 - acc: 0.7539     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.5118 - acc: 0.7461     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.5138 - acc: 0.7500     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.5004 - acc: 0.7604     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4977 - acc: 0.7669     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4975 - acc: 0.7591     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.5174 - acc: 0.7500     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4938 - acc: 0.7604     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.5261 - acc: 0.7357     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4911 - acc: 0.7552     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.5133 - acc: 0.7500     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4905 - acc: 0.7708     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4925 - acc: 0.7682     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4891 - acc: 0.7539     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4940 - acc: 0.7526     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.5080 - acc: 0.7578     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4871 - acc: 0.7708     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4897 - acc: 0.7643     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4882 - acc: 0.7773     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4961 - acc: 0.7604     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4937 - acc: 0.7643     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4889 - acc: 0.7734     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4825 - acc: 0.7721     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4783 - acc: 0.7760     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4773 - acc: 0.7682     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.5030 - acc: 0.7565     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4864 - acc: 0.7656     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4912 - acc: 0.7630     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4834 - acc: 0.7695     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4908 - acc: 0.7578     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4794 - acc: 0.7682     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4840 - acc: 0.7813     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4905 - acc: 0.7656     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4747 - acc: 0.7786     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4738 - acc: 0.7773     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4863 - acc: 0.7669     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4847 - acc: 0.7643     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4954 - acc: 0.7747     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4890 - acc: 0.7578     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4866 - acc: 0.7747     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4841 - acc: 0.7604     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4711 - acc: 0.7747     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4886 - acc: 0.7643     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4839 - acc: 0.7630     \n",
      " 32/768 [>.............................] - ETA: 0s\n",
      "acc: 78.52%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# 고정 시드 값으로 초기화\n",
    "numpy.random.seed(123)\n",
    "\n",
    "\n",
    "\n",
    "# Pima Indians Diabetes 데이터 불러오기\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# 입력 변수와 출력 변수를 구분하기\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# 모형 적합\n",
    "model.fit(X, Y, epochs=150, batch_size=10)\n",
    "\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
